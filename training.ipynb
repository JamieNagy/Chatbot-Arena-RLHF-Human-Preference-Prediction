{
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 66631,
          "databundleVersionId": 8346466,
          "sourceType": "competition"
        },
        {
          "sourceId": 8377405,
          "sourceType": "datasetVersion",
          "datasetId": 4959805
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 594.352474,
      "end_time": "2024-07-11T07:32:02.541544",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-07-11T07:22:08.189070",
      "version": "2.5.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RTX6000 ADA <br>\n",
        "training will take approximatly 16 hrs"
      ],
      "metadata": {
        "id": "T6faKSMXM-sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gemma-2 is available from transformers>=4.42.3\n",
        "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "papermill": {
          "duration": 29.502078,
          "end_time": "2024-07-11T07:22:40.359028",
          "exception": false,
          "start_time": "2024-07-11T07:22:10.856950",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "DK5aIOvJM-se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq datasets\n",
        "!pip install -qq scikit-learn"
      ],
      "metadata": {
        "id": "QTQiwlwAM-se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    Gemma2ForSequenceClassification,\n",
        "    GemmaTokenizerFast,\n",
        "    Gemma2Config,\n",
        "    PreTrainedTokenizerBase,\n",
        "    EvalPrediction,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from sklearn.metrics import log_loss, accuracy_score"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 19.39064,
          "end_time": "2024-07-11T07:22:59.761341",
          "exception": false,
          "start_time": "2024-07-11T07:22:40.370701",
          "status": "completed"
        },
        "tags": [],
        "id": "ASmqhh-TM-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010104,
          "end_time": "2024-07-11T07:22:59.781961",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.771857",
          "status": "completed"
        },
        "tags": [],
        "id": "QZNFJsD2M-sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    output_dir: str = \"output\"\n",
        "    checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
        "    max_length: int = 3120\n",
        "    n_splits: int = 10\n",
        "    fold_idx: int = 0\n",
        "    optim_type: str = \"adamw_hf\"\n",
        "    per_device_train_batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 6\n",
        "    per_device_eval_batch_size: int = 8\n",
        "    n_epochs: int = 1\n",
        "    freeze_layers: int = 0\n",
        "    lr: float = 1e-4\n",
        "    warmup_steps: int = 20\n",
        "    lora_r: int = 128\n",
        "    lora_alpha: float = lora_r * 1\n",
        "    lora_dropout: float = 0\n",
        "    lora_bias: str = \"none\"\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "editable": true,
        "papermill": {
          "duration": 0.020577,
          "end_time": "2024-07-11T07:22:59.812606",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.792029",
          "status": "completed"
        },
        "tags": [],
        "id": "843ir_UsM-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Arguments"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.009879,
          "end_time": "2024-07-11T07:22:59.832748",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.822869",
          "status": "completed"
        },
        "tags": [],
        "id": "PQR3oQi7M-sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_0 = TrainingArguments(\n",
        "    output_dir=\"output-first\",\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\",\n",
        "    num_train_epochs=config.n_epochs,\n",
        "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1800,\n",
        "    optim=config.optim_type,\n",
        "    fp16=True,\n",
        "    learning_rate=config.lr,\n",
        "    warmup_steps=config.warmup_steps,\n",
        ")"
      ],
      "metadata": {
        "editable": true,
        "papermill": {
          "duration": 0.074241,
          "end_time": "2024-07-11T07:22:59.918039",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.843798",
          "status": "completed"
        },
        "tags": [],
        "id": "vHOisSjSM-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoRA config"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.009852,
          "end_time": "2024-07-11T07:22:59.938160",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.928308",
          "status": "completed"
        },
        "tags": [],
        "id": "2GzlU1JyM-sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=config.lora_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    # only target self-attention\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\",\"gate_proj\"],\n",
        "    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
        "    lora_dropout=config.lora_dropout,\n",
        "    bias=config.lora_bias,\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017924,
          "end_time": "2024-07-11T07:22:59.966078",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.948154",
          "status": "completed"
        },
        "tags": [],
        "id": "h6_6LSRdM-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the tokenizer & model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.009988,
          "end_time": "2024-07-11T07:22:59.985967",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.975979",
          "status": "completed"
        },
        "tags": [],
        "id": "9oYPMFGQM-sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\n",
        "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "papermill": {
          "duration": 4.152643,
          "end_time": "2024-07-11T07:23:04.148816",
          "exception": false,
          "start_time": "2024-07-11T07:22:59.996173",
          "status": "completed"
        },
        "tags": [],
        "id": "Cot0l4c4M-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Gemma2ForSequenceClassification.from_pretrained(\n",
        "    config.checkpoint,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "model"
      ],
      "metadata": {
        "papermill": {
          "duration": 218.312,
          "end_time": "2024-07-11T07:26:42.472174",
          "exception": false,
          "start_time": "2024-07-11T07:23:04.160174",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "LBuPi265M-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "YuvfmZXoM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the dataset"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01131,
          "end_time": "2024-07-11T07:26:42.531641",
          "exception": false,
          "start_time": "2024-07-11T07:26:42.520331",
          "status": "completed"
        },
        "tags": [],
        "id": "MpfphLFxM-sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase, max_length: int) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch: dict) -> dict:\n",
        "        # Pre-process texts\n",
        "        prompt = [\"<prompt>: \" + self.process_prompt(t) for t in batch[\"prompt\"]]\n",
        "        response_a = [\"\\n\\n<response_a>: \" + self.process_response(t) for t in batch[\"response_a\"]]\n",
        "        response_b = [\"\\n\\n<response_b>: \" + self.process_response(t) for t in batch[\"response_b\"]]\n",
        "\n",
        "        # Custom concatenation and truncation logic\n",
        "        texts = []\n",
        "        for p_parts, r_a_parts, r_b_parts, p, r_a, r_b in zip(batch[\"prompt\"], batch[\"response_a\"], batch[\"response_b\"], prompt, response_a, response_b):\n",
        "            full_text = p + r_a + r_b\n",
        "            full_text_tokens = self.tokenizer.tokenize(full_text)\n",
        "            total_length = len(full_text_tokens)\n",
        "\n",
        "            if total_length <= self.max_length:\n",
        "                texts.append(full_text)  # directly use the concatenated text\n",
        "            else:\n",
        "                # Calculate proportions\n",
        "                len_p = len(self.tokenizer.tokenize(p))\n",
        "                len_r_a = len(self.tokenizer.tokenize(r_a))\n",
        "                len_r_b = len(self.tokenizer.tokenize(r_b))\n",
        "\n",
        "                total_part_length = len_p + len_r_a + len_r_b\n",
        "                proportion_p = len_p / total_part_length\n",
        "                proportion_r_a = len_r_a / total_part_length\n",
        "                proportion_r_b = len_r_b / total_part_length\n",
        "\n",
        "                # Calculate tokens to keep per part\n",
        "                tokens_to_keep_p = int(proportion_p * self.max_length)\n",
        "                tokens_to_keep_r_a = int(proportion_r_a * self.max_length)\n",
        "                tokens_to_keep_r_b = int(proportion_r_b * self.max_length)\n",
        "\n",
        "                # function to be completed truncate_parts\n",
        "                tokens_p = self.truncate_parts(p_parts,tokens_to_keep_p, \"<prompt>: \", \"prompt\")\n",
        "                tokens_r_a = self.truncate_parts(r_a_parts,tokens_to_keep_r_a, \"\\n\\n<response_a>: \", \"response\")\n",
        "                tokens_r_b = self.truncate_parts(r_b_parts,tokens_to_keep_r_b, \"\\n\\n<response_b>: \", \"response\")\n",
        "\n",
        "                texts.append(tokens_p + tokens_r_a + tokens_r_b)\n",
        "\n",
        "        # Final tokenization step - ensure texts are in the right format (list of strings)\n",
        "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True, padding=False)\n",
        "\n",
        "        # Processing labels\n",
        "        labels = [0 if a_win else 1 if b_win else 2 for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"])]\n",
        "        return {**tokenized, \"labels\": labels}\n",
        "\n",
        "    def truncate_parts(self, parts_text, max_length_for_segment, prefix, tag):\n",
        "        # Evaluate the string representation of the list into an actual list\n",
        "        parts = eval(parts_text, {\"null\": \"\"})\n",
        "\n",
        "        # Tokenize each part separately and store tokens\n",
        "        tokenized_parts = [self.tokenizer.tokenize(f\"\\n{tag} {i+1}: \" + part) for i, part in enumerate(parts)]\n",
        "        # Calculate the length of tokens for each part and the total length\n",
        "        part_lengths = [len(tokens) for tokens in tokenized_parts]\n",
        "        total_parts_length = sum(part_lengths)\n",
        "\n",
        "        # Calculate the proportional maximum length for each part\n",
        "        part_max_lengths = [(length / total_parts_length) * max_length_for_segment if total_parts_length > 0 else max_length_for_segment / len(parts) for length in part_lengths]\n",
        "        # Truncate each part to its proportional length and decode\n",
        "        truncated_parts = []\n",
        "        for tokens, max_len in zip(tokenized_parts, part_max_lengths):\n",
        "            truncated_tokens = tokens[:int(max_len)]\n",
        "            token_ids = self.tokenizer.convert_tokens_to_ids(truncated_tokens)\n",
        "            truncated_text = self.tokenizer.decode(token_ids)\n",
        "            truncated_parts.append(truncated_text)\n",
        "\n",
        "        # Concatenate all truncated parts\n",
        "        final_text = prefix + \" \".join(truncated_parts)\n",
        "        return final_text\n",
        "\n",
        "    def process_prompt(self, text: str) -> str:\n",
        "        parts = eval(text, {\"null\": \"\"})\n",
        "        return \"\".join(f\"\\nprompt {i+1}: {part}\" for i, part in enumerate(parts))\n",
        "\n",
        "    def process_response(self, text: str) -> str:\n",
        "        parts = eval(text, {\"null\": \"\"})\n",
        "        return \"\".join(f\"\\nresponse {i+1}: {part}\" for i, part in enumerate(parts))"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.023466,
          "end_time": "2024-07-11T07:26:45.915124",
          "exception": false,
          "start_time": "2024-07-11T07:26:45.891658",
          "status": "completed"
        },
        "tags": [],
        "id": "hxDlGlThM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = CustomTokenizer(tokenizer, max_length=config.max_length)"
      ],
      "metadata": {
        "id": "bhlSmMPhM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_extra = Dataset.from_csv(\"my_input/lmsys-additional-33k-labelled-conversations/lmsys-33k-deduplicated.csv\")\n",
        "ds_extra = ds_extra.map(encode, batched=True)"
      ],
      "metadata": {
        "id": "Vpvc5_sMM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_csv(\"my_input/lmsys-chatbot-arena/train.csv\")\n",
        "ds = ds.map(encode, batched=True)"
      ],
      "metadata": {
        "id": "zL_MMdLTM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute metrics\n",
        "\n",
        "We'll compute the log-loss used in LB and accuracy as a auxiliary metric."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011769,
          "end_time": "2024-07-11T07:26:46.542515",
          "exception": false,
          "start_time": "2024-07-11T07:26:46.530746",
          "status": "completed"
        },
        "tags": [],
        "id": "t9loChP0M-sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
        "    preds = eval_preds.predictions\n",
        "    labels = eval_preds.label_ids\n",
        "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
        "    loss = log_loss(y_true=labels, y_pred=probs)\n",
        "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
        "    return {\"acc\": acc, \"log_loss\": loss}"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01987,
          "end_time": "2024-07-11T07:26:46.574012",
          "exception": false,
          "start_time": "2024-07-11T07:26:46.554142",
          "status": "completed"
        },
        "tags": [],
        "id": "l4T6lBoqM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split\n",
        "\n",
        "Here, train and eval is splitted according to their `id % 5`"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011453,
          "end_time": "2024-07-11T07:26:46.597153",
          "exception": false,
          "start_time": "2024-07-11T07:26:46.585700",
          "status": "completed"
        },
        "tags": [],
        "id": "OYMx9yALM-sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folds = [\n",
        "    (\n",
        "        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
        "        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
        "    )\n",
        "    for fold_idx in range(config.n_splits)\n",
        "]"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.019576,
          "end_time": "2024-07-11T07:26:46.628437",
          "exception": false,
          "start_time": "2024-07-11T07:26:46.608861",
          "status": "completed"
        },
        "tags": [],
        "id": "yvGJrVBKM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx, eval_idx = folds[config.fold_idx]\n",
        "\n",
        "trainer = Trainer(\n",
        "    args=training_args_0,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=ds_extra,\n",
        "    eval_dataset=ds.select(eval_idx),\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "editable": true,
        "papermill": {
          "duration": 313.007115,
          "end_time": "2024-07-11T07:31:59.647151",
          "exception": false,
          "start_time": "2024-07-11T07:26:46.640036",
          "status": "completed"
        },
        "tags": [],
        "id": "uRzcCUVTM-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_1 = TrainingArguments(\n",
        "    output_dir=\"output-second\",\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\",\n",
        "    num_train_epochs=config.n_epochs,\n",
        "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
        "    logging_steps=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1550,\n",
        "    optim=config.optim_type,\n",
        "    fp16=True,\n",
        "    learning_rate=1.5e-5,\n",
        "    warmup_steps=20,\n",
        ")"
      ],
      "metadata": {
        "id": "f-O_MmyLM-sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    args=training_args_1,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=ds.select(train_idx),\n",
        "    eval_dataset=ds.select(eval_idx),\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qDnyEhjNM-sh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}