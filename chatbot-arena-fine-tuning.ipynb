{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c5c357",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-07-29T16:34:57.620697Z",
          "iopub.status.busy": "2024-07-29T16:34:57.620105Z",
          "iopub.status.idle": "2024-07-29T16:35:15.602501Z",
          "shell.execute_reply": "2024-07-29T16:35:15.601385Z"
        },
        "papermill": {
          "duration": 17.991956,
          "end_time": "2024-07-29T16:35:15.605014",
          "exception": false,
          "start_time": "2024-07-29T16:34:57.613058",
          "status": "completed"
        },
        "tags": [],
        "id": "35c5c357",
        "outputId": "d4a32ee7-d52c-407c-8e03-c1a11799b20e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: /kaggle/input/lmsys-wheel-files\r\n",
            "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\r\n",
            "Processing /kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\r\n",
            "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\r\n",
            "Processing /kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\r\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\r\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
            "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\r\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\r\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\r\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\r\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\r\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\r\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\r\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
            "Installing collected packages: bitsandbytes, peft\r\n",
            "Successfully installed bitsandbytes-0.43.1 peft-0.11.1\r\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5dec32",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:35:15.636036Z",
          "iopub.status.busy": "2024-07-29T16:35:15.635729Z",
          "iopub.status.idle": "2024-07-29T16:35:34.463950Z",
          "shell.execute_reply": "2024-07-29T16:35:34.463001Z"
        },
        "papermill": {
          "duration": 18.839094,
          "end_time": "2024-07-29T16:35:34.466223",
          "exception": false,
          "start_time": "2024-07-29T16:35:15.627129",
          "status": "completed"
        },
        "tags": [],
        "id": "7c5dec32",
        "outputId": "b08fcd14-e3c6-42ee-fa9b-6a85f86c3706"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-29 16:35:23.748325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-29 16:35:23.748428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-29 16:35:23.858682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import torch\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    Gemma2ForSequenceClassification, GemmaTokenizerFast,\n",
        "    AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
        ")\n",
        "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
        "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "torch.backends.cuda.enable_flash_sdp(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9aff966",
      "metadata": {
        "papermill": {
          "duration": 0.007207,
          "end_time": "2024-07-29T16:35:34.481352",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.474145",
          "status": "completed"
        },
        "tags": [],
        "id": "a9aff966"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "#### Directory Paths:\n",
        " - `gemma_dir`: Directory path for the Gemma-2 model.\n",
        " - `gemma_lora_dir`: Directory path for the Gemma LoRA checkpoint. - `llama_model_name`: Directory path for the Llama model.\n",
        " - `llama_weights_path`: Directory path for the Llama model weights.\n",
        "\n",
        "#### Other Parameters:\n",
        " - `max_length`: Maximum sequence length.\n",
        " - `batch_size`: Batch size for processing.\n",
        " - `tta`: Boolean flag for test-time augmentation.\n",
        " - `spread_max_length`: Boolean flag for spreading max length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec407dc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:35:34.497971Z",
          "iopub.status.busy": "2024-07-29T16:35:34.497387Z",
          "iopub.status.idle": "2024-07-29T16:35:34.503851Z",
          "shell.execute_reply": "2024-07-29T16:35:34.502906Z"
        },
        "papermill": {
          "duration": 0.016967,
          "end_time": "2024-07-29T16:35:34.505759",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.488792",
          "status": "completed"
        },
        "tags": [],
        "id": "bec407dc"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    gemma_dir: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
        "    gemma_lora_dir: str = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
        "    llama_model_name: str = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
        "    llama_weights_path: str = '/kaggle/input/lmsys-model/model'\n",
        "    max_length: int = 2048\n",
        "    batch_size: int = 4\n",
        "    tta: bool = False\n",
        "    spread_max_length: bool = False\n",
        "\n",
        "# Instantiate the configuration\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a611bf79",
      "metadata": {
        "papermill": {
          "duration": 0.007001,
          "end_time": "2024-07-29T16:35:34.520030",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.513029",
          "status": "completed"
        },
        "tags": [],
        "id": "a611bf79"
      },
      "source": [
        "# Load and Process Test Data\n",
        "\n",
        "#### 🔧 Functions and Operations:\n",
        " - `process_text`: A function to process text data by evaluating and joining the text.\n",
        " - Apply the `process_text` function to the 'prompt', 'response_a', and 'response_b' columns of the test DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30820c6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:35:34.535791Z",
          "iopub.status.busy": "2024-07-29T16:35:34.535137Z",
          "iopub.status.idle": "2024-07-29T16:35:34.557261Z",
          "shell.execute_reply": "2024-07-29T16:35:34.556585Z"
        },
        "papermill": {
          "duration": 0.031897,
          "end_time": "2024-07-29T16:35:34.559224",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.527327",
          "status": "completed"
        },
        "tags": [],
        "id": "e30820c6"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
        "\n",
        "def process_text(text: str) -> str:\n",
        "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
        "\n",
        "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
        "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
        "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21eaa67e",
      "metadata": {
        "papermill": {
          "duration": 0.007057,
          "end_time": "2024-07-29T16:35:34.573519",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.566462",
          "status": "completed"
        },
        "tags": [],
        "id": "21eaa67e"
      },
      "source": [
        "# Tokenize Function\n",
        "\n",
        " - The function handles different tokenization formats based on the type of tokenizer.\n",
        " - It also handles whether to spread the maximum length or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d60755",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:35:34.589061Z",
          "iopub.status.busy": "2024-07-29T16:35:34.588792Z",
          "iopub.status.idle": "2024-07-29T16:35:34.598338Z",
          "shell.execute_reply": "2024-07-29T16:35:34.597543Z"
        },
        "papermill": {
          "duration": 0.019652,
          "end_time": "2024-07-29T16:35:34.600267",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.580615",
          "status": "completed"
        },
        "tags": [],
        "id": "22d60755"
      },
      "outputs": [],
      "source": [
        "def tokenize(tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):\n",
        "    # Handle different formats for different tokenizers\n",
        "    if isinstance(tokenizer, GemmaTokenizerFast):\n",
        "        prompt = [\"<prompt>: \" + p for p in prompt]\n",
        "        response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
        "        response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
        "    else:\n",
        "        prompt = [\"User prompt: \" + p for p in prompt]\n",
        "        response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n",
        "        response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n",
        "\n",
        "    # Tokenize with spread max length\n",
        "    if spread_max_length:\n",
        "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
        "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
        "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
        "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
        "        attention_mask = [[1] * len(i) for i in input_ids]\n",
        "    # Tokenize without spread max length\n",
        "    else:\n",
        "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
        "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
        "        input_ids = tokenized.input_ids\n",
        "        attention_mask = tokenized.attention_mask\n",
        "\n",
        "    return input_ids, attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab7cd4e3",
      "metadata": {
        "papermill": {
          "duration": 0.006936,
          "end_time": "2024-07-29T16:35:34.615345",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.608409",
          "status": "completed"
        },
        "tags": [],
        "id": "ab7cd4e3"
      },
      "source": [
        "# Tokenizer Setup and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f7a9cb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:35:34.630793Z",
          "iopub.status.busy": "2024-07-29T16:35:34.630509Z",
          "iopub.status.idle": "2024-07-29T16:35:36.261635Z",
          "shell.execute_reply": "2024-07-29T16:35:36.260702Z"
        },
        "papermill": {
          "duration": 1.641376,
          "end_time": "2024-07-29T16:35:36.263853",
          "exception": false,
          "start_time": "2024-07-29T16:35:34.622477",
          "status": "completed"
        },
        "tags": [],
        "id": "a6f7a9cb",
        "outputId": "106c0c0c-74c9-481d-a031-fbc3755a5636"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Gemma Tokenizer\n",
        "gemma_tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
        "gemma_tokenizer.add_eos_token = True\n",
        "gemma_tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Llama Tokenizer\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
        "\n",
        "# Prepare data for Gemma model\n",
        "gemma_data = pd.DataFrame()\n",
        "gemma_data[\"id\"] = test[\"id\"]\n",
        "gemma_data[\"input_ids\"], gemma_data[\"attention_mask\"] = tokenize(gemma_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
        "gemma_data[\"length\"] = gemma_data[\"input_ids\"].apply(len)\n",
        "\n",
        "# Prepare data for Llama model\n",
        "llama_data = pd.DataFrame()\n",
        "llama_data[\"id\"] = test[\"id\"]\n",
        "llama_data[\"input_ids\"], llama_data[\"attention_mask\"] = tokenize(llama_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
        "llama_data[\"length\"] = llama_data[\"input_ids\"].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab394147",
      "metadata": {
        "papermill": {
          "duration": 0.0072,
          "end_time": "2024-07-29T16:35:36.278562",
          "exception": false,
          "start_time": "2024-07-29T16:35:36.271362",
          "status": "completed"
        },
        "tags": [],
        "id": "ab394147"
      },
      "source": [
        "# Load Gemma Model on GPU 0\n",
        "\n",
        "\n",
        "  Apply PEFT using the LoRA checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baab7966",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:35:36.294097Z",
          "iopub.status.busy": "2024-07-29T16:35:36.293809Z",
          "iopub.status.idle": "2024-07-29T16:36:56.485714Z",
          "shell.execute_reply": "2024-07-29T16:36:56.484920Z"
        },
        "papermill": {
          "duration": 80.202336,
          "end_time": "2024-07-29T16:36:56.488061",
          "exception": false,
          "start_time": "2024-07-29T16:35:36.285725",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "938f9221c7bd4444a6f1249f120a9c70"
          ]
        },
        "id": "baab7966",
        "outputId": "6e781710-c5cd-49be-d751-69ca783a97a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "938f9221c7bd4444a6f1249f120a9c70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Gemma model on GPU 0\n",
        "device_0 = torch.device('cuda:0')\n",
        "# Load the Gemma model for sequence classification onto GPU 0\n",
        "gemma_model = Gemma2ForSequenceClassification.from_pretrained(\n",
        "    cfg.gemma_dir,\n",
        "    device_map=device_0,\n",
        "    use_cache=False\n",
        ")\n",
        "# Apply PEFT using the LoRA checkpoint\n",
        "gemma_model = PeftModel.from_pretrained(gemma_model, cfg.gemma_lora_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac1a071",
      "metadata": {
        "papermill": {
          "duration": 0.007288,
          "end_time": "2024-07-29T16:36:56.503160",
          "exception": false,
          "start_time": "2024-07-29T16:36:56.495872",
          "status": "completed"
        },
        "tags": [],
        "id": "7ac1a071"
      },
      "source": [
        "# Load Llama Model on GPU 1\n",
        "\n",
        "Loads the Llama model for sequence classification onto GPU 1 and applies parameter-efficient fine-tuning (PEFT) using LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39c4ea01",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:36:56.519105Z",
          "iopub.status.busy": "2024-07-29T16:36:56.518808Z",
          "iopub.status.idle": "2024-07-29T16:38:42.205780Z",
          "shell.execute_reply": "2024-07-29T16:38:42.204907Z"
        },
        "papermill": {
          "duration": 105.697453,
          "end_time": "2024-07-29T16:38:42.208027",
          "exception": false,
          "start_time": "2024-07-29T16:36:56.510574",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "aa9c3d60240a4092bac7ba022cdb2fb9"
          ]
        },
        "id": "39c4ea01",
        "outputId": "012007ea-6f7f-426f-c835-ef726e21075e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa9c3d60240a4092bac7ba022cdb2fb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama-3/transformers/8b-chat-hf/1 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForSequenceClassification(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear8bitLt(\n",
              "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear8bitLt(\n",
              "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (score): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=3, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Llama model on GPU 1\n",
        "device_1 = torch.device('cuda:1')\n",
        "\n",
        "# Configure BitsAndBytes for 8-bit loading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16,\n",
        "    bnb_8bit_use_double_quant=False\n",
        ")\n",
        "#  Load the Llama model for sequence classification onto GPU 1\n",
        "llama_base_model = LlamaForSequenceClassification.from_pretrained(\n",
        "    cfg.llama_model_name,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='cuda:1')\n",
        "llama_base_model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
        "\n",
        "# Configure and apply PEFT using the LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.10,\n",
        "    bias='none',\n",
        "    inference_mode=True,\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    target_modules=['o_proj', 'v_proj']\n",
        ")\n",
        "llama_model = get_peft_model(llama_base_model, peft_config).to(device_1)\n",
        "llama_model.load_state_dict(torch.load(cfg.llama_weights_path), strict=False)\n",
        "llama_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0e416d",
      "metadata": {
        "papermill": {
          "duration": 0.007813,
          "end_time": "2024-07-29T16:38:42.224023",
          "exception": false,
          "start_time": "2024-07-29T16:38:42.216210",
          "status": "completed"
        },
        "tags": [],
        "id": "dc0e416d"
      },
      "source": [
        "# Inference Function\n",
        "\n",
        " - The function uses mixed precision (automatic casting) for faster and more efficient inference.\n",
        " - It uses a no_grad context to avoid computing gradients, which saves memory and computation time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b6da60",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:38:42.241382Z",
          "iopub.status.busy": "2024-07-29T16:38:42.241064Z",
          "iopub.status.idle": "2024-07-29T16:38:42.249825Z",
          "shell.execute_reply": "2024-07-29T16:38:42.249035Z"
        },
        "papermill": {
          "duration": 0.019462,
          "end_time": "2024-07-29T16:38:42.251661",
          "exception": false,
          "start_time": "2024-07-29T16:38:42.232199",
          "status": "completed"
        },
        "tags": [],
        "id": "e3b6da60"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "@torch.cuda.amp.autocast()\n",
        "def inference(df, model, tokenizer, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
        "    a_win, b_win, tie = [], [], []\n",
        "    # Process the data in batches\n",
        "    for start_idx in range(0, len(df), batch_size):\n",
        "        end_idx = min(start_idx + batch_size, len(df))\n",
        "        tmp = df.iloc[start_idx:end_idx]\n",
        "        input_ids = tmp[\"input_ids\"].to_list()\n",
        "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
        "        # Pad the inputs without triggering the fast tokenizer warning\n",
        "        inputs = pad_without_fast_tokenizer_warning(\n",
        "            tokenizer,\n",
        "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
        "            padding=\"longest\",\n",
        "            pad_to_multiple_of=None,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Perform inference\n",
        "        outputs = model(**inputs.to(device))\n",
        "        proba = outputs.logits.softmax(-1).cpu()\n",
        "\n",
        "        # Append the probabilities to the corresponding lists\n",
        "        a_win.extend(proba[:, 0].tolist())\n",
        "        b_win.extend(proba[:, 1].tolist())\n",
        "        tie.extend(proba[:, 2].tolist())\n",
        "    # Update the DataFrame with the probabilities\n",
        "    df[\"winner_model_a\"] = a_win\n",
        "    df[\"winner_model_b\"] = b_win\n",
        "    df[\"winner_tie\"] = tie\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9134cb7d",
      "metadata": {
        "papermill": {
          "duration": 0.007745,
          "end_time": "2024-07-29T16:38:42.267296",
          "exception": false,
          "start_time": "2024-07-29T16:38:42.259551",
          "status": "completed"
        },
        "tags": [],
        "id": "9134cb7d"
      },
      "source": [
        "# Perform Inference Using Threading\n",
        "\n",
        "Using a ThreadPoolExecutor to run inference on both models in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8ab2c7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:38:42.284223Z",
          "iopub.status.busy": "2024-07-29T16:38:42.283963Z",
          "iopub.status.idle": "2024-07-29T16:38:48.885465Z",
          "shell.execute_reply": "2024-07-29T16:38:48.884511Z"
        },
        "papermill": {
          "duration": 6.612501,
          "end_time": "2024-07-29T16:38:48.887662",
          "exception": false,
          "start_time": "2024-07-29T16:38:42.275161",
          "status": "completed"
        },
        "tags": [],
        "id": "0b8ab2c7",
        "outputId": "41320b29-8135-43df-eb7e-047be3ecb152"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference time: 6.59 seconds\n"
          ]
        }
      ],
      "source": [
        "st = time.time()\n",
        "\n",
        "# Sort data by input length\n",
        "gemma_data = gemma_data.sort_values(\"length\", ascending=False)\n",
        "llama_data = llama_data.sort_values(\"length\", ascending=False)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    results = executor.map(inference,\n",
        "                           (gemma_data, llama_data),\n",
        "                           (gemma_model, llama_model),\n",
        "                           (gemma_tokenizer, llama_tokenizer),\n",
        "                           (device_0, device_1))\n",
        "\n",
        "gemma_result_df, llama_result_df = list(results)\n",
        "\n",
        "# Combine results (simple average)\n",
        "combined_result_df = gemma_result_df.copy()\n",
        "combined_result_df[\"winner_model_a\"] = (gemma_result_df[\"winner_model_a\"] + llama_result_df[\"winner_model_a\"]) / 2\n",
        "combined_result_df[\"winner_model_b\"] = (gemma_result_df[\"winner_model_b\"] + llama_result_df[\"winner_model_b\"]) / 2\n",
        "combined_result_df[\"winner_tie\"] = (gemma_result_df[\"winner_tie\"] + llama_result_df[\"winner_tie\"]) / 2\n",
        "\n",
        "print(f\"Inference time: {time.time() - st:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe40a9b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-29T16:38:48.922241Z",
          "iopub.status.busy": "2024-07-29T16:38:48.921951Z",
          "iopub.status.idle": "2024-07-29T16:38:48.940023Z",
          "shell.execute_reply": "2024-07-29T16:38:48.939216Z"
        },
        "papermill": {
          "duration": 0.029268,
          "end_time": "2024-07-29T16:38:48.942106",
          "exception": false,
          "start_time": "2024-07-29T16:38:48.912838",
          "status": "completed"
        },
        "tags": [],
        "id": "9fe40a9b",
        "outputId": "035b32ad-cec4-4214-c4c3-326a057f41bf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>winner_model_a</th>\n",
              "      <th>winner_model_b</th>\n",
              "      <th>winner_tie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1233961</td>\n",
              "      <td>0.188499</td>\n",
              "      <td>0.534701</td>\n",
              "      <td>0.276801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>211333</td>\n",
              "      <td>0.350908</td>\n",
              "      <td>0.268337</td>\n",
              "      <td>0.380755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>136060</td>\n",
              "      <td>0.068339</td>\n",
              "      <td>0.797322</td>\n",
              "      <td>0.134339</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  winner_model_a  winner_model_b  winner_tie\n",
              "2  1233961        0.188499        0.534701    0.276801\n",
              "1   211333        0.350908        0.268337    0.380755\n",
              "0   136060        0.068339        0.797322    0.134339"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "submission_df = combined_result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "display(submission_df.head())"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 8346466,
          "sourceId": 66631,
          "sourceType": "competition"
        },
        {
          "datasetId": 5034873,
          "sourceId": 8449074,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5297895,
          "sourceId": 8897601,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5369301,
          "sourceId": 8926343,
          "sourceType": "datasetVersion"
        },
        {
          "modelId": 39106,
          "modelInstanceId": 28083,
          "sourceId": 33551,
          "sourceType": "modelInstanceVersion"
        },
        {
          "modelId": 86587,
          "modelInstanceId": 63082,
          "sourceId": 75103,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30747,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 237.232355,
      "end_time": "2024-07-29T16:38:52.124943",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-07-29T16:34:54.892588",
      "version": "2.5.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}